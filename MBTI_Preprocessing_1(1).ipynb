{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import libraries'''\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows and columns of data (8675, 2) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8675 entries, 0 to 8674\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    8675 non-null   object\n",
      " 1   posts   8675 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 67.8+ KB\n",
      "\n",
      "Check for missing values type     0\n",
      "posts    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Load csv file'''\n",
    "data = pd.read_csv(\"mbti_1.csv\")\n",
    "\n",
    "\n",
    "'''find number of rows and columns of data'''\n",
    "print(\"the number of rows and columns of data\",data.shape,'\\n')\n",
    "\n",
    "'''View information about the dataset'''\n",
    "data.info()\n",
    "\n",
    "\n",
    "'''observe posts content'''\n",
    "#print(data.iloc[0,1])\n",
    "\n",
    "'''Check for missing values'''\n",
    "print()\n",
    "print(\"Check for missing values\", data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8675</td>\n",
       "      <td>8675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>16</td>\n",
       "      <td>8675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'Always striving for more interesting possibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                              posts\n",
       "count   8675                                               8675\n",
       "unique    16                                               8675\n",
       "top     INFP  'Always striving for more interesting possibil...\n",
       "freq    1832                                                  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
       "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Find the types of personality'''\n",
    "data['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFP    1832\n",
       "INFJ    1470\n",
       "INTP    1304\n",
       "INTJ    1091\n",
       "ENTP     685\n",
       "ENFP     675\n",
       "ISTP     337\n",
       "ISFP     271\n",
       "ENTJ     231\n",
       "ISTJ     205\n",
       "ENFJ     190\n",
       "ISFJ     166\n",
       "ESTP      89\n",
       "ESFP      48\n",
       "ESFJ      42\n",
       "ESTJ      39\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Find the count of each personality type'''\n",
    "category_count = data['type'].value_counts()\n",
    "category_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = data.copy()\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocessing_regex(text):\n",
    "    text=re.sub(r'\\|\\|\\|', r'', text) # removing |||\n",
    "    text=re.sub(r'http\\S+', r'', text) # removing http.\n",
    "    text=text.replace(\".\", \"\").replace(\",\",\"\").replace('+', '').replace(\":\",\"\").replace('\"','').replace(\"'\",'')\n",
    "    text=text.replace(\";\",'').replace('?',\"\").replace('_____','').replace('-','').replace('/','').replace(\"@\",\"\")\n",
    "    text=text.replace(\"!\",'')\n",
    "    text=re.sub(r'[()]', '', text) # removing '(' or ')'\n",
    "    text=re.sub(r'[<>]', '', text) # removing '<' or '>'\n",
    "    text=text.replace(\"*\",\"\").lower()\n",
    "    text=re.sub(\"[^a-zA-Z]\",\" \",text) #keep only words\n",
    "    prepocessed_text=text\n",
    "    \n",
    "    stop_words = (stopwords.words('english')) \n",
    "                                               # mbti types will be removed in the posts for training and test \n",
    "    stop_words.extend(('infj', 'entp', 'intp', 'intj', 'entj', 'enfj', 'infp', \n",
    "                   'enfp', 'isfp', 'istp', 'isfj', 'istj', 'estp', 'esfp', 'estj', 'esfj',\n",
    "                   'INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP','ISFP', \n",
    "                   'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'))\n",
    "    stop_words=set(stop_words)\n",
    "    word_tokens=word_tokenize(prepocessed_text)\n",
    "    word_tokenized_preprocessed=[]\n",
    "\n",
    "    for token in word_tokens:\n",
    "        if token not in stop_words:\n",
    "            word_tokenized_preprocessed.append(token)\n",
    "            \n",
    "    s=word_tokenized_preprocessed\n",
    "    converting_stopwords = ' '.join(map(str, s))   # converting list to string\n",
    "    stop_words=[converting_stopwords]\n",
    "    \n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmantization(prepocessed_words):\n",
    "    n=WordNetLemmatizer()\n",
    "    words_for_lemman=prepocessed_words\n",
    "    lemmantized_word=[n.lemmatize(w) for w in words_for_lemman]           \n",
    "    to_str=lemmantized_word\n",
    "    lemmantized_word=' '.join(map(str, to_str))   # converting list to string\n",
    "    \n",
    "    return lemmantized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snow_stemmer(prepocessed_words):\n",
    "    snow_stemmer = SnowballStemmer(language='english')\n",
    "    words_for_snow_stemmer= prepocessed_words\n",
    "    snowballstemmed_words=[snow_stemmer.stem(w) for w in words_for_snow_stemmer]\n",
    "    return snowballstemmed_words\n",
    "    \n",
    "def stemmer_counter(text):\n",
    "    out=CountVectorizer(text)    \n",
    "    return out\n",
    "\n",
    "def stemmer_tf_idf(text):\n",
    "    out=TfidfVectorizer(text)   \n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lem_counter(text):\n",
    "    out=CountVectorizer(text)    \n",
    "    return out\n",
    "\n",
    "def lem_tf_idf(text):\n",
    "    out=TfidfVectorizer(text)   \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed</th>\n",
       "      <th>lemmantized</th>\n",
       "      <th>snow_stemmer</th>\n",
       "      <th>lem_counter</th>\n",
       "      <th>lem_tf_idf</th>\n",
       "      <th>stemmer_counter</th>\n",
       "      <th>stemmer_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>CountVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>TfidfVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>CountVectorizer(input=['moments sportscenter t...</td>\n",
       "      <td>TfidfVectorizer(input=['moments sportscenter t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>CountVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>TfidfVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>CountVectorizer(input=['im finding lack posts ...</td>\n",
       "      <td>TfidfVectorizer(input=['im finding lack posts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>CountVectorizer(input='good one course say kno...</td>\n",
       "      <td>TfidfVectorizer(input='good one course say kno...</td>\n",
       "      <td>CountVectorizer(input=['good one course say kn...</td>\n",
       "      <td>TfidfVectorizer(input=['good one course say kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>CountVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>TfidfVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>CountVectorizer(input=['dear enjoyed conversat...</td>\n",
       "      <td>TfidfVectorizer(input=['dear enjoyed conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>CountVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>TfidfVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>CountVectorizer(input=['youre firedthats anoth...</td>\n",
       "      <td>TfidfVectorizer(input=['youre firedthats anoth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lemmantized  \\\n",
       "0  moments sportscenter top ten plays prankswhat ...   \n",
       "1  im finding lack posts alarmingsex boring posit...   \n",
       "2  good one course say know thats blessing cursed...   \n",
       "3  dear enjoyed conversation day esoteric gabbing...   \n",
       "4  youre firedthats another silly misconception a...   \n",
       "\n",
       "                                        snow_stemmer  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lem_counter  \\\n",
       "0  CountVectorizer(input='moments sportscenter to...   \n",
       "1  CountVectorizer(input='im finding lack posts a...   \n",
       "2  CountVectorizer(input='good one course say kno...   \n",
       "3  CountVectorizer(input='dear enjoyed conversati...   \n",
       "4  CountVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                          lem_tf_idf  \\\n",
       "0  TfidfVectorizer(input='moments sportscenter to...   \n",
       "1  TfidfVectorizer(input='im finding lack posts a...   \n",
       "2  TfidfVectorizer(input='good one course say kno...   \n",
       "3  TfidfVectorizer(input='dear enjoyed conversati...   \n",
       "4  TfidfVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                     stemmer_counter  \\\n",
       "0  CountVectorizer(input=['moments sportscenter t...   \n",
       "1  CountVectorizer(input=['im finding lack posts ...   \n",
       "2  CountVectorizer(input=['good one course say kn...   \n",
       "3  CountVectorizer(input=['dear enjoyed conversat...   \n",
       "4  CountVectorizer(input=['youre firedthats anoth...   \n",
       "\n",
       "                                       stemmer_tfidf  \n",
       "0  TfidfVectorizer(input=['moments sportscenter t...  \n",
       "1  TfidfVectorizer(input=['im finding lack posts ...  \n",
       "2  TfidfVectorizer(input=['good one course say kn...  \n",
       "3  TfidfVectorizer(input=['dear enjoyed conversat...  \n",
       "4  TfidfVectorizer(input=['youre firedthats anoth...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# texts are preprocessed and stop words are removed\n",
    "dataframe['processed'] = dataframe['posts'].apply(prepocessing_regex)\n",
    "\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed</th>\n",
       "      <th>lemmantized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lemmantized  \n",
       "0  moments sportscenter top ten plays prankswhat ...  \n",
       "1  im finding lack posts alarmingsex boring posit...  \n",
       "2  good one course say know thats blessing cursed...  \n",
       "3  dear enjoyed conversation day esoteric gabbing...  \n",
       "4  youre firedthats another silly misconception a...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying lemmatization\n",
    "dataframe['lemmantized'] = dataframe['processed'].apply(Lemmantization)\n",
    "dataframe.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying stemmer\n",
    "dataframe['snow_stemmer'] = dataframe['processed'].apply(snow_stemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed</th>\n",
       "      <th>lemmantized</th>\n",
       "      <th>snow_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lemmantized  \\\n",
       "0  moments sportscenter top ten plays prankswhat ...   \n",
       "1  im finding lack posts alarmingsex boring posit...   \n",
       "2  good one course say know thats blessing cursed...   \n",
       "3  dear enjoyed conversation day esoteric gabbing...   \n",
       "4  youre firedthats another silly misconception a...   \n",
       "\n",
       "                                        snow_stemmer  \n",
       "0  [moments sportscenter top ten plays prankswhat...  \n",
       "1  [im finding lack posts alarmingsex boring posi...  \n",
       "2  [good one course say know thats blessing curse...  \n",
       "3  [dear enjoyed conversation day esoteric gabbin...  \n",
       "4  [youre firedthats another silly misconception ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'moments': 134, 'sportscenter': 203, 'top': 227, 'ten': 212, 'plays': 159, 'prankswhat': 163, 'lifechanging': 117, 'experience': 66, 'life': 116, 'repeat': 173, 'todaymay': 224, 'perc': 154, 'immerse': 99, 'youthe': 258, 'last': 108, 'thing': 213, 'friend': 75, 'posted': 162, 'facebook': 67, 'committing': 31, 'suicide': 208, 'next': 142, 'day': 44, 'rest': 175, 'peace': 151, 'sorry': 200, 'hear': 90, 'distress': 53, 'natural': 140, 'relationship': 172, 'perfection': 155, 'time': 222, 'every': 62, 'moment': 133, 'existence': 65, 'try': 229, 'figure': 72, 'hard': 86, 'times': 223, 'growth': 83, 'welcome': 242, 'stuff': 206, 'game': 77, 'set': 187, 'matchprozac': 125, 'wellbrutin': 243, 'least': 113, 'thirty': 217, 'minutes': 131, 'moving': 136, 'legs': 115, 'dont': 54, 'mean': 127, 'sitting': 193, 'desk': 49, 'chair': 25, 'weed': 241, 'moderation': 132, 'maybe': 126, 'edibles': 59, 'healthier': 89, 'alternativebasically': 2, 'come': 30, 'three': 220, 'items': 103, 'youve': 259, 'determined': 50, 'type': 231, 'whichever': 246, 'types': 232, 'want': 238, 'would': 252, 'likely': 119, 'use': 233, 'given': 79, 'cognitive': 29, 'functions': 76, 'whatnot': 244, 'left': 114, 'byall': 22, 'things': 215, 'sims': 192, 'indeed': 100, 'video': 236, 'good': 80, 'one': 146, 'note': 143, 'somewhat': 199, 'subjective': 207, 'completely': 32, 'promoting': 165, 'death': 45, 'simdear': 191, 'favorite': 71, 'games': 78, 'growing': 82, 'current': 42, 'cool': 38, 'appears': 4, 'late': 109, 'sadtheres': 182, 'someone': 197, 'everyonewait': 64, 'thought': 218, 'confidence': 34, 'thingi': 214, 'cherish': 26, 'solitude': 196, 'bc': 14, 'revel': 176, 'within': 248, 'inner': 101, 'world': 250, 'whereas': 245, 'id': 96, 'workin': 249, 'enjoy': 60, 'worry': 251, 'people': 153, 'always': 3, 'around': 6, 'toyo': 228, 'ladies': 107, 'youre': 257, 'complimentary': 33, 'personalitywell': 157, 'hey': 93, 'main': 121, 'social': 194, 'outlet': 148, 'xbox': 253, 'live': 120, 'conversations': 36, 'even': 61, 'verbally': 235, 'fatigue': 70, 'quickly': 168, 'really': 170, 'dig': 52, 'part': 150, 'thread': 219, 'requires': 174, 'meget': 128, 'high': 94, 'backyard': 12, 'roast': 179, 'eat': 58, 'marshmellows': 123, 'conversing': 37, 'something': 198, 'intellectual': 102, 'followed': 73, 'massages': 124, 'kisses': 106, 'many': 122, 'bs': 21, 'sentence': 186, 'could': 40, 'think': 216, 'bbanned': 13, 'watching': 239, 'movies': 135, 'corner': 39, 'duncesbanned': 57, 'health': 88, 'class': 27, 'clearly': 28, 'taught': 211, 'nothing': 144, 'peer': 152, 'pressurebanned': 164, 'whole': 247, 'host': 95, 'reasons': 171, 'two': 230, 'baby': 11, 'deer': 46, 'right': 177, 'munching': 138, 'beetle': 17, 'middle': 130, 'using': 234, 'blood': 20, 'cavemen': 24, 'diary': 51, 'todays': 225, 'latest': 110, 'happenings': 85, 'designated': 48, 'cave': 23, 'wall': 237, 'see': 184, 'asa': 9, 'pokemon': 160, 'society': 195, 'everyone': 63, 'becomes': 15, 'optimist': 147, 'artists': 8, 'draw': 56, 'idea': 97, 'counts': 41, 'forming': 74, 'like': 118, 'signaturewelcome': 190, 'robot': 180, 'ranks': 169, 'person': 156, 'downed': 55, 'selfesteem': 185, 'cuz': 43, 'im': 98, 'avid': 10, 'signature': 189, 'artist': 7, 'proudbanned': 166, 'taking': 210, 'room': 181, 'bed': 16, 'ya': 254, 'got': 81, 'ta': 209, 'learn': 111, 'share': 188, 'roaches': 178, 'much': 137, 'thundering': 221, 'grumbling': 84, 'kind': 105, 'storm': 205, 'yepahh': 256, 'old': 145, 'school': 183, 'music': 139, 'havent': 87, 'heard': 91, 'ages': 0, 'failed': 68, 'public': 167, 'speaking': 202, 'years': 255, 'ago': 1, 'ive': 104, 'sort': 201, 'learned': 112, 'better': 18, 'position': 161, 'big': 19, 'failure': 69, 'overloading': 149, 'tooi': 226, 'persons': 158, 'mentality': 129, 'hes': 92, 'confirmed': 35, 'way': 240, 'denver': 47, 'area': 5, 'start': 204, 'new': 141}\n",
      "(1, 260)\n",
      "[[1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2\n",
      "  1 1 1 1 1 2 2 2 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 2 1 2 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1\n",
      "  1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 4 2 1 1 1 1 1 2 2 1 2 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      "  1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorization result using stemmer\n",
    "\n",
    "def Print_stemmer_counter(text): \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    print(vectorizer.vocabulary_)# encode document\n",
    "    vector = vectorizer.transform(text)\n",
    "    # summarize encoded vector\n",
    "    print(vector.shape)\n",
    "    print(vector.toarray())\n",
    "    \n",
    "\n",
    "Print_stemmer_counter(dataframe['processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider clean data\n",
    "\n",
    "df = dataframe[['type', 'lemmantized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed</th>\n",
       "      <th>lemmantized</th>\n",
       "      <th>snow_stemmer</th>\n",
       "      <th>lem_counter</th>\n",
       "      <th>lem_tf_idf</th>\n",
       "      <th>stemmer_counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>CountVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>TfidfVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>CountVectorizer(input=['moments sportscenter t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>CountVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>TfidfVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>CountVectorizer(input=['im finding lack posts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>CountVectorizer(input='good one course say kno...</td>\n",
       "      <td>TfidfVectorizer(input='good one course say kno...</td>\n",
       "      <td>CountVectorizer(input=['good one course say kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>CountVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>TfidfVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>CountVectorizer(input=['dear enjoyed conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>CountVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>TfidfVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>CountVectorizer(input=['youre firedthats anoth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lemmantized  \\\n",
       "0  moments sportscenter top ten plays prankswhat ...   \n",
       "1  im finding lack posts alarmingsex boring posit...   \n",
       "2  good one course say know thats blessing cursed...   \n",
       "3  dear enjoyed conversation day esoteric gabbing...   \n",
       "4  youre firedthats another silly misconception a...   \n",
       "\n",
       "                                        snow_stemmer  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lem_counter  \\\n",
       "0  CountVectorizer(input='moments sportscenter to...   \n",
       "1  CountVectorizer(input='im finding lack posts a...   \n",
       "2  CountVectorizer(input='good one course say kno...   \n",
       "3  CountVectorizer(input='dear enjoyed conversati...   \n",
       "4  CountVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                          lem_tf_idf  \\\n",
       "0  TfidfVectorizer(input='moments sportscenter to...   \n",
       "1  TfidfVectorizer(input='im finding lack posts a...   \n",
       "2  TfidfVectorizer(input='good one course say kno...   \n",
       "3  TfidfVectorizer(input='dear enjoyed conversati...   \n",
       "4  TfidfVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                     stemmer_counter  \n",
       "0  CountVectorizer(input=['moments sportscenter t...  \n",
       "1  CountVectorizer(input=['im finding lack posts ...  \n",
       "2  CountVectorizer(input=['good one course say kn...  \n",
       "3  CountVectorizer(input=['dear enjoyed conversat...  \n",
       "4  CountVectorizer(input=['youre firedthats anoth...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['lem_counter'] = dataframe['lemmantized'].apply(lem_counter)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe['lem_tf_idf'] = dataframe['lemmantized'].apply(lem_tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe['stemmer_counter'] = dataframe['snow_stemmer'].apply(stemmer_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>processed</th>\n",
       "      <th>lemmantized</th>\n",
       "      <th>snow_stemmer</th>\n",
       "      <th>lem_counter</th>\n",
       "      <th>lem_tf_idf</th>\n",
       "      <th>stemmer_counter</th>\n",
       "      <th>stemmer_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "      <td>[moments sportscenter top ten plays prankswhat...</td>\n",
       "      <td>CountVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>TfidfVectorizer(input='moments sportscenter to...</td>\n",
       "      <td>CountVectorizer(input=['moments sportscenter t...</td>\n",
       "      <td>TfidfVectorizer(input=['moments sportscenter t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "      <td>[im finding lack posts alarmingsex boring posi...</td>\n",
       "      <td>CountVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>TfidfVectorizer(input='im finding lack posts a...</td>\n",
       "      <td>CountVectorizer(input=['im finding lack posts ...</td>\n",
       "      <td>TfidfVectorizer(input=['im finding lack posts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "      <td>[good one course say know thats blessing curse...</td>\n",
       "      <td>CountVectorizer(input='good one course say kno...</td>\n",
       "      <td>TfidfVectorizer(input='good one course say kno...</td>\n",
       "      <td>CountVectorizer(input=['good one course say kn...</td>\n",
       "      <td>TfidfVectorizer(input=['good one course say kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "      <td>[dear enjoyed conversation day esoteric gabbin...</td>\n",
       "      <td>CountVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>TfidfVectorizer(input='dear enjoyed conversati...</td>\n",
       "      <td>CountVectorizer(input=['dear enjoyed conversat...</td>\n",
       "      <td>TfidfVectorizer(input=['dear enjoyed conversat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "      <td>[youre firedthats another silly misconception ...</td>\n",
       "      <td>CountVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>TfidfVectorizer(input='youre firedthats anothe...</td>\n",
       "      <td>CountVectorizer(input=['youre firedthats anoth...</td>\n",
       "      <td>TfidfVectorizer(input=['youre firedthats anoth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts  \\\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4  ENTJ  'You're fired.|||That's another silly misconce...   \n",
       "\n",
       "                                           processed  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lemmantized  \\\n",
       "0  moments sportscenter top ten plays prankswhat ...   \n",
       "1  im finding lack posts alarmingsex boring posit...   \n",
       "2  good one course say know thats blessing cursed...   \n",
       "3  dear enjoyed conversation day esoteric gabbing...   \n",
       "4  youre firedthats another silly misconception a...   \n",
       "\n",
       "                                        snow_stemmer  \\\n",
       "0  [moments sportscenter top ten plays prankswhat...   \n",
       "1  [im finding lack posts alarmingsex boring posi...   \n",
       "2  [good one course say know thats blessing curse...   \n",
       "3  [dear enjoyed conversation day esoteric gabbin...   \n",
       "4  [youre firedthats another silly misconception ...   \n",
       "\n",
       "                                         lem_counter  \\\n",
       "0  CountVectorizer(input='moments sportscenter to...   \n",
       "1  CountVectorizer(input='im finding lack posts a...   \n",
       "2  CountVectorizer(input='good one course say kno...   \n",
       "3  CountVectorizer(input='dear enjoyed conversati...   \n",
       "4  CountVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                          lem_tf_idf  \\\n",
       "0  TfidfVectorizer(input='moments sportscenter to...   \n",
       "1  TfidfVectorizer(input='im finding lack posts a...   \n",
       "2  TfidfVectorizer(input='good one course say kno...   \n",
       "3  TfidfVectorizer(input='dear enjoyed conversati...   \n",
       "4  TfidfVectorizer(input='youre firedthats anothe...   \n",
       "\n",
       "                                     stemmer_counter  \\\n",
       "0  CountVectorizer(input=['moments sportscenter t...   \n",
       "1  CountVectorizer(input=['im finding lack posts ...   \n",
       "2  CountVectorizer(input=['good one course say kn...   \n",
       "3  CountVectorizer(input=['dear enjoyed conversat...   \n",
       "4  CountVectorizer(input=['youre firedthats anoth...   \n",
       "\n",
       "                                       stemmer_tfidf  \n",
       "0  TfidfVectorizer(input=['moments sportscenter t...  \n",
       "1  TfidfVectorizer(input=['im finding lack posts ...  \n",
       "2  TfidfVectorizer(input=['good one course say kn...  \n",
       "3  TfidfVectorizer(input=['dear enjoyed conversat...  \n",
       "4  TfidfVectorizer(input=['youre firedthats anoth...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['stemmer_tfidf'] = dataframe['snow_stemmer'].apply(stemmer_tf_idf)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X will be chosen by us from 4 cases.\n",
    "#X= dataframe['stemmer_tfidf'] or dataframe['stemmer_counter'] or dataframe['lem_counter'] or dataframe['lem_tf_idf']\n",
    "\n",
    "# y will be label \n",
    "#y=dataframe['type'] \n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>lemmantized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>moments sportscenter top ten plays prankswhat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im finding lack posts alarmingsex boring posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>good one course say know thats blessing cursed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>dear enjoyed conversation day esoteric gabbing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>youre firedthats another silly misconception a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                        lemmantized\n",
       "0  INFJ  moments sportscenter top ten plays prankswhat ...\n",
       "1  ENTP  im finding lack posts alarmingsex boring posit...\n",
       "2  INTP  good one course say know thats blessing cursed...\n",
       "3  INTJ  dear enjoyed conversation day esoteric gabbing...\n",
       "4  ENTJ  youre firedthats another silly misconception a..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consider clean data\n",
    "\n",
    "df = dataframe[['type', 'lemmantized']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after X are confirmed, we will find optimized parameter of chosen X according to the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer(\n",
    "                             #analyzer = 'word',\n",
    "                             tokenizer=None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             #ngram_range=(1, 3), \n",
    "                             max_df=.8,\n",
    "                             min_df=3,                        \n",
    "                             max_features = 2000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  vect.fit_transform(df[\"lemmantized\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<6072x2000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 1582858 stored elements in Compressed Sparse Row format>,\n",
       " <2603x2000 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 674954 stored elements in Compressed Sparse Row format>,\n",
       " array([ 9,  8,  9, ...,  2, 11,  4]),\n",
       " array([ 2, 15, 11, ...,  3,  3,  9]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
